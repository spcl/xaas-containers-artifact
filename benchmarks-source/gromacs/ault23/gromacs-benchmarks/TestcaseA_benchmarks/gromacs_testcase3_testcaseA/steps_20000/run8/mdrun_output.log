                      :-) GROMACS - gmx mdrun, 2025.0 (-:

Executable:   /usr/local/gromacs/bin/gmx_mpi
Data prefix:  /usr/local/gromacs
Working dir:  /result_dir/run8
Command line:
  gmx_mpi mdrun -s /input_dir/ion_channel.tpr -ntomp 64 -nsteps 20000 -resethway


Back Off! I just backed up md.log to ./#md.log.1#
Reading file /input_dir/ion_channel.tpr, VERSION 2020.3 (single precision)
Note: file tpx version 119, software tpx version 137
Overriding nsteps with value passed on the command line: 20000 steps, 50 ps
Changing nstlist from 10 to 100, rlist from 1 to 1.169


Update groups can not be used for this system because there are three or more consecutively coupled constraints

GPU-aware MPI was not detected, will not use direct GPU communication. Check the GROMACS install guide for recommendations for GPU-aware support. If you are certain about GPU-aware support in your MPI library, you can force its use by setting the GMX_FORCE_GPU_AWARE_MPI environment variable.

1 GPU selected for this run.
Mapping of GPU IDs to the 2 GPU tasks in the 1 rank on this node:
  PP:0,PME:0
PP tasks will do (non-perturbed) short-ranged interactions on the GPU
PP task will update and constrain coordinates on the CPU
PME tasks will do all aspects on the GPU
Using 1 MPI process
Using 64 OpenMP threads 


Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Protein'
20000 steps,     50.0 ps.

step 10000: resetting all time and cycle counters

Writing final coordinates.

Back Off! I just backed up confout.gro to ./#confout.gro.1#

               Core t (s)   Wall t (s)        (%)
       Time:     1126.138       17.618     6391.9
                 (ns/day)    (hour/ns)
Performance:      122.613        0.196

GROMACS reminds you: "The public have an insatiable curiosity to know everything, except what is worth knowing." (Oscar Wilde)

