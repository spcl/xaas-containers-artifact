                      :-) GROMACS - gmx mdrun, 2024.4 (-:

Executable:   /users/ealnuaim/spack/opt/spack/linux-neoverse_v2/gromacs-2024.4-2yteqaulwasocucvvtgipmpdbb5r5fck/bin/gmx_mpi
Data prefix:  /users/ealnuaim/spack/opt/spack/linux-neoverse_v2/gromacs-2024.4-2yteqaulwasocucvvtgipmpdbb5r5fck
Working dir:  /users/ealnuaim/gromacs_benchmarks/TestcaseA_benchmarks/gromacs_testcase5_testcaseA/run_29
Command line:
  gmx_mpi mdrun -s /users/ealnuaim/GROMACS_TestCaseA/ion_channel.tpr -ntomp 64 -bonded gpu -nb gpu -pme gpu -pin on -v -noconfout -dlb yes -nstlist 300 -gpu_id 0 -npme 1 -nsteps 300

Reading file /users/ealnuaim/GROMACS_TestCaseA/ion_channel.tpr, VERSION 2020.3 (single precision)
Note: file tpx version 119, software tpx version 134
This run has forced use of 'GPU-aware MPI'. However, GROMACS cannot determine if underlying MPI is GPU-aware. Check the GROMACS install guide for recommendations for GPU-aware support. If you observe failures at runtime, try unsetting the GMX_FORCE_GPU_AWARE_MPI environment variable.

GMX_ENABLE_DIRECT_GPU_COMM environment variable detected, enabling direct GPU communication using GPU-aware MPI.

Overriding nsteps with value passed on the command line: 300 steps, 0.75 ps
Changing nstlist from 10 to 300, rlist from 1 to 1.81


Update groups can not be used for this system because there are three or more consecutively coupled constraints

On host nid006912 1 GPU selected for this run.
Mapping of GPU IDs to the 4 GPU tasks in the 4 ranks on this node:
  PP:0,PP:0,PP:0,PME:0
PP tasks will do (non-perturbed) short-ranged and most bonded interactions on the GPU
PP task will update and constrain coordinates on the CPU
PME tasks will do all aspects on the GPU
GPU direct communication will be used between MPI ranks.
Using 4 MPI processes
Using 64 OpenMP threads per MPI process


Overriding thread affinity set outside gmx mdrun
starting mdrun 'Protein'
300 steps,      0.8 ps.
step 0
step 100
step 200
vol 1.00  imb F  2% pme/F 1.42 step 300, remaining wall clock time:     0 s          


Dynamic load balancing report:
 DLB was permanently on during the run per user request.
 Average load imbalance: 2.5%.
 The balanceable part of the MD step is 11%, load imbalance is computed from this.
 Part of the total run time spent waiting due to load imbalance: 0.3%.
 Steps where the load balancing was limited by -rdd, -rcon and/or -dds: Z 0 %
 Average PME mesh/force load: 1.421
 Part of the total run time spent waiting due to PP/PME imbalance: 10.2 %

NOTE: 10.2 % performance was lost because the PME ranks
      had more work to do than the PP ranks.
      You might want to increase the number of PME ranks
      or increase the cut-off and the grid spacing.


               Core t (s)   Wall t (s)        (%)
       Time:      208.558        0.818    25497.8
                 (ns/day)    (hour/ns)
Performance:       79.487        0.302

GROMACS reminds you: "I had a polynomial once. My doctor removed it." (Michael Grant)

