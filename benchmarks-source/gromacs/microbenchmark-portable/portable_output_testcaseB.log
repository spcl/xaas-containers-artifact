                      :-) GROMACS - gmx mdrun, 2025.0 (-:

Executable:   /usr/local/gromacs/bin/gmx_mpi
Data prefix:  /usr/local/gromacs
Working dir:  /
Command line:
  gmx_mpi mdrun -s /data/gromacs/GROMACS_TestCaseB/lignocellulose.tpr -ntmpi 1 -ntomp 64 -gpu_id 0 -nsteps 100

Compiled SIMD is SSE4.1, but AVX2_256 might be faster (see log).
Reading file /data/gromacs/GROMACS_TestCaseB/lignocellulose.tpr, VERSION 2020.1 (single precision)
Note: file tpx version 119, software tpx version 137

-------------------------------------------------------
Program:     gmx mdrun, version 2025.0
Source file: src/gromacs/taskassignment/resourcedivision.cpp (line 725)

Fatal error:
Setting the number of thread-MPI ranks is only supported with thread-MPI and
GROMACS was compiled without thread-MPI

For more information and tips for troubleshooting, please check the GROMACS
website at https://manual.gromacs.org/current/user-guide/run-time-errors.html
-------------------------------------------------------
Abort(1) on node 0 (rank 0 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 1) - process 0
mcopik@ault25:/$ HYDRA_LAUNCHER=fork mpiexec -n 1 /usr/local/gromacs/bin/gmx_mpi mdrun -s /data/gromacs/GROMACS_TestCaseB/lignocellulose.tpr -ntomp 64 -gpu_id 0 -nsteps 100
                      :-) GROMACS - gmx mdrun, 2025.0 (-:

Executable:   /usr/local/gromacs/bin/gmx_mpi
Data prefix:  /usr/local/gromacs
Working dir:  /
Command line:
  gmx_mpi mdrun -s /data/gromacs/GROMACS_TestCaseB/lignocellulose.tpr -ntomp 64 -gpu_id 0 -nsteps 100


Back Off! I just backed up md.log to ./#md.log.1#
Compiled SIMD is SSE4.1, but AVX2_256 might be faster (see log).
Reading file /data/gromacs/GROMACS_TestCaseB/lignocellulose.tpr, VERSION 2020.1 (single precision)
Note: file tpx version 119, software tpx version 137
Overriding nsteps with value passed on the command line: 100 steps, 0.2 ps
Changing nstlist from 10 to 25, rlist from 1.262 to 1.407


Update groups can not be used for this system because there are three or more consecutively coupled constraints

GPU-aware MPI was not detected, will not use direct GPU communication. Check the GROMACS install guide for recommendations for GPU-aware support. If you are certain about GPU-aware support in your MPI library, you can force its use by setting the GMX_FORCE_GPU_AWARE_MPI environment variable.

1 GPU selected for this run.
Mapping of GPU IDs to the 1 GPU task in the 1 rank on this node:
  PP:0
PP tasks will do (non-perturbed) short-ranged interactions on the GPU
PP task will update and constrain coordinates on the GPU
Using 1 MPI process
Using 64 OpenMP threads


WARNING: On rank 0: oversubscribing the recommended max load of 1 logical CPUs per node with 64 threads.
         This will cause considerable performance loss.

NOTE: Oversubscribing available/permitted CPUs, will not pin threads
starting mdrun ''
100 steps,      0.2 ps.

UR CUDA ERROR:
        Value:           1
        Name:            CUDA_ERROR_INVALID_VALUE
        Description:     invalid argument
        Function:        urEnqueueKernelLaunch
        Source Location: /tmp/tmp.HhqPmzG672/intel-llvm-mirror/build/_deps/unified-runtime-src/source/adapters/cuda/enqueue.cpp:458


-------------------------------------------------------
Program:     gmx mdrun, version 2025.0

Unknown exception:
(exception type: N4sycl3_V114nd_range_errorE)
Number of work-groups exceed limit for dimension 2 : 65663 > 65535 -30
(PI_ERROR_INVALID_VALUE)

For more information and tips for troubleshooting, please check the GROMACS
website at https://manual.gromacs.org/current/user-guide/run-time-errors.html
-------------------------------------------------------
Abort(1) on node 0 (rank 0 in comm 0): application called MPI_Abort(MPI_COMM_WORLD, 1) - process 0
