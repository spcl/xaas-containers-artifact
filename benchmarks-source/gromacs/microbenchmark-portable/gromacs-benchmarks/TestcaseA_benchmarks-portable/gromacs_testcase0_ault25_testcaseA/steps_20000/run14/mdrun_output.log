                      :-) GROMACS - gmx mdrun, 2025.0 (-:

Executable:   /usr/local/gromacs/bin/gmx_mpi
Data prefix:  /usr/local/gromacs
Working dir:  /result_dir/run14
Command line:
  gmx_mpi mdrun -s /input_dir/ion_channel.tpr -ntomp 128 -nsteps 20000 -resetstep 10000

Reading file /input_dir/ion_channel.tpr, VERSION 2020.3 (single precision)
Note: file tpx version 119, software tpx version 137
Overriding nsteps with value passed on the command line: 20000 steps, 50 ps
Changing nstlist from 10 to 100, rlist from 1 to 1.169


Update groups can not be used for this system because there are three or more consecutively coupled constraints

GPU-aware MPI was not detected, will not use direct GPU communication. Check the GROMACS install guide for recommendations for GPU-aware support. If you are certain about GPU-aware support in your MPI library, you can force its use by setting the GMX_FORCE_GPU_AWARE_MPI environment variable.

1 GPU selected for this run.
Mapping of GPU IDs to the 2 GPU tasks in the 1 rank on this node:
  PP:0,PME:0
PP tasks will do (non-perturbed) short-ranged interactions on the GPU
PP task will update and constrain coordinates on the CPU
PME tasks will do all aspects on the GPU
Using 1 MPI process
Using 128 OpenMP threads 

starting mdrun 'Protein'
20000 steps,     50.0 ps.

step 10000: resetting all time and cycle counters

Writing final coordinates.

               Core t (s)   Wall t (s)        (%)
       Time:     2451.552       19.281    12714.8
                 (ns/day)    (hour/ns)
Performance:      112.038        0.214

GROMACS reminds you: "We all understand the twinge of discomfort at the thought that we share a common ancestor with the apes. No one can embarrass you like a relative." (Neal DeGrasse Tyson)

