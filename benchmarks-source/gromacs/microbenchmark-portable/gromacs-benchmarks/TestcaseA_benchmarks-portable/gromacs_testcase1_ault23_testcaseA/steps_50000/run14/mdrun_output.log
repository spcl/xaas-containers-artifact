                      :-) GROMACS - gmx mdrun, 2025.0 (-:

Executable:   /usr/local/gromacs/bin/gmx_mpi
Data prefix:  /usr/local/gromacs
Working dir:  /result_dir/run14
Command line:
  gmx_mpi mdrun -s /input_dir/ion_channel.tpr -ntomp 64 -nsteps 50000 -resetstep 10000

Compiled SIMD is SSE4.1, but AVX_512 might be faster (see log).
Reading file /input_dir/ion_channel.tpr, VERSION 2020.3 (single precision)
Note: file tpx version 119, software tpx version 137
Overriding nsteps with value passed on the command line: 50000 steps, 125 ps
Changing nstlist from 10 to 100, rlist from 1 to 1.169


Update groups can not be used for this system because there are three or more consecutively coupled constraints

GPU-aware MPI was not detected, will not use direct GPU communication. Check the GROMACS install guide for recommendations for GPU-aware support. If you are certain about GPU-aware support in your MPI library, you can force its use by setting the GMX_FORCE_GPU_AWARE_MPI environment variable.

1 GPU selected for this run.
Mapping of GPU IDs to the 2 GPU tasks in the 1 rank on this node:
  PP:0,PME:0
PP tasks will do (non-perturbed) short-ranged interactions on the GPU
PP task will update and constrain coordinates on the CPU
PME tasks will do all aspects on the GPU
Using 1 MPI process
Using 64 OpenMP threads 

starting mdrun 'Protein'
50000 steps,    125.0 ps.

step 10000: resetting all time and cycle counters

Writing final coordinates.

               Core t (s)   Wall t (s)        (%)
       Time:     5203.598       81.359     6395.8
                 (ns/day)    (hour/ns)
Performance:      106.199        0.226

GROMACS reminds you: "We must be clear that when it comes to atoms, language can be used only as in poetry. " (Niels Bohr)

