An instance of this daemon is already running
An instance of this daemon is already running
An instance of this daemon is already running
                   :-) GROMACS - gmx mdrun, 2025.0-spack (-:

Executable:   /iopsstor/scratch/cscs/ealnuaim/spack/opt/spack/linux-neoverse_v2/gromacs-2025.0-ipfxhzxv53y7pxoqwb6rqy32vdtare4i/bin/gmx_mpi
Data prefix:  /iopsstor/scratch/cscs/ealnuaim/spack/opt/spack/linux-neoverse_v2/gromacs-2025.0-ipfxhzxv53y7pxoqwb6rqy32vdtare4i
Working dir:  /iopsstor/scratch/cscs/ealnuaim/gromacs_benchmarks/TestcaseB_benchmarks/gromacs_spack_2025/run_13
Command line:
  gmx_mpi mdrun -s /iopsstor/scratch/cscs/ealnuaim/GROMACS_TestCaseB/lignocellulose.tpr -ntomp 64 -bonded gpu -nb gpu -pme cpu -pin on -v -noconfout -dlb yes -nstlist 300 -gpu_id 0 -nsteps 300

Reading file /iopsstor/scratch/cscs/ealnuaim/GROMACS_TestCaseB/lignocellulose.tpr, VERSION 2020.1 (single precision)
Note: file tpx version 119, software tpx version 137
Overriding nsteps with value passed on the command line: 300 steps, 0.6 ps
Changing nstlist from 10 to 300, rlist from 1.262 to 2.416


Update groups can not be used for this system because there are three or more consecutively coupled constraints

GPU-aware MPI was not detected, will not use direct GPU communication. Check the GROMACS install guide for recommendations for GPU-aware support. If you are certain about GPU-aware support in your MPI library, you can force its use by setting the GMX_FORCE_GPU_AWARE_MPI environment variable.

On host nid006043 1 GPU selected for this run.
Mapping of GPU IDs to the 4 GPU tasks in the 4 ranks on this node:
  PP:0,PP:0,PP:0,PP:0
PP tasks will do (non-perturbed) short-ranged and most bonded interactions on the GPU
PP task will update and constrain coordinates on the CPU
Using 4 MPI processes
Using 64 OpenMP threads per MPI process


Overriding thread affinity set outside gmx mdrun
starting mdrun ''
300 steps,      0.6 ps.
step 0
step 100
step 200
vol 1.00  imb F  5% step 300, remaining wall clock time:     0 s          


Dynamic load balancing report:
 DLB was permanently on during the run per user request.
 Average load imbalance: 4.5%.
 The balanceable part of the MD step is 64%, load imbalance is computed from this.
 Part of the total run time spent waiting due to load imbalance: 2.9%.
 Steps where the load balancing was limited by -rdd, -rcon and/or -dds: X 0 %


               Core t (s)   Wall t (s)        (%)
       Time:      313.453        1.227    25545.2
                 (ns/day)    (hour/ns)
Performance:       42.388        0.566

GROMACS reminds you: "Do not quench your inspiration and your imagination; do not become the slave of your model." (Vincent Van Gogh)

Cannot send command to MPS control daemon process
Cannot send command to MPS control daemon process
Cannot send command to MPS control daemon process
