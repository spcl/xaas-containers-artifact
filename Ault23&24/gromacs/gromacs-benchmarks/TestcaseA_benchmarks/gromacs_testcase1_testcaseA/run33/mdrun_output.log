                      :-) GROMACS - gmx mdrun, 2025.0 (-:

Executable:   /users/ealnuaim/native_build/gromacs-2025.0/bin/gmx_mpi
Data prefix:  /users/ealnuaim/native_build/gromacs-2025.0
Working dir:  /users/ealnuaim
Command line:
  gmx_mpi mdrun -s /users/ealnuaim/GROMACS_TestCaseA/ion_channel.tpr -ntomp 64 -gpu_id 0 -nsteps 100

Compiled SIMD is AVX2_256, but CPU also supports AVX_512 (see log).
Reading file /users/ealnuaim/GROMACS_TestCaseA/ion_channel.tpr, VERSION 2020.3 (single precision)
Note: file tpx version 119, software tpx version 137
Overriding nsteps with value passed on the command line: 100 steps, 0.25 ps
Changing nstlist from 10 to 100, rlist from 1 to 1.169


Update groups can not be used for this system because there are three or more consecutively coupled constraints

GPU-aware MPI was not detected, will not use direct GPU communication. Check the GROMACS install guide for recommendations for GPU-aware support. If you are certain about GPU-aware support in your MPI library, you can force its use by setting the GMX_FORCE_GPU_AWARE_MPI environment variable.

1 GPU selected for this run.
Mapping of GPU IDs to the 2 GPU tasks in the 1 rank on this node:
  PP:0,PME:0
PP tasks will do (non-perturbed) short-ranged interactions on the GPU
PP task will update and constrain coordinates on the CPU
PME tasks will do all aspects on the GPU
Using 1 MPI process
Using 64 OpenMP threads 

starting mdrun 'Protein'
100 steps,      0.2 ps.

Writing final coordinates.

               Core t (s)   Wall t (s)        (%)
       Time:       33.800        0.530     6382.3
                 (ns/day)    (hour/ns)
Performance:       41.194        0.583

GROMACS reminds you: "If you open your mind too much, your brains will fall out." (Tim Minchin)

