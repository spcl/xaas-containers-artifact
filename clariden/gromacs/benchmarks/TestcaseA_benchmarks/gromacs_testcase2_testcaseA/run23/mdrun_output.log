/usr/local/gromacs/bin/gmx_mpi: /usr/lib/aarch64-linux-gnu/libnl-3.so.200: no version information available (required by /usr/lib64/libcxi.so.1)
/usr/local/gromacs/bin/gmx_mpi: /usr/lib/aarch64-linux-gnu/libnl-3.so.200: no version information available (required by /usr/lib64/libcxi.so.1)
/usr/local/gromacs/bin/gmx_mpi: /usr/lib/aarch64-linux-gnu/libnl-3.so.200: no version information available (required by /usr/lib64/libcxi.so.1)
/usr/local/gromacs/bin/gmx_mpi: /usr/lib/aarch64-linux-gnu/libnl-3.so.200: no version information available (required by /usr/lib64/libcxi.so.1)
                      :-) GROMACS - gmx mdrun, 2025.0 (-:

Executable:   /usr/local/gromacs/bin/gmx_mpi
Data prefix:  /usr/local/gromacs
Working dir:  /users/ealnuaim/gromacs_benchmarks/TestcaseA_benchmarks/gromacs_testcase2_testcaseA/run_23
Command line:
  gmx_mpi mdrun -s /users/ealnuaim/GROMACS_TestCaseA/ion_channel.tpr -ntomp 64 -bonded gpu -nb gpu -pme gpu -pin on -v -noconfout -dlb yes -nstlist 300 -gpu_id 0 -npme 1 -nsteps 300

Compiled SIMD is ARM_NEON_ASIMD, but ARM_SVE might be faster (see log).
Reading file /users/ealnuaim/GROMACS_TestCaseA/ion_channel.tpr, VERSION 2020.3 (single precision)
Note: file tpx version 119, software tpx version 137
Overriding nsteps with value passed on the command line: 300 steps, 0.75 ps
Changing nstlist from 10 to 300, rlist from 1 to 1.81


Update groups can not be used for this system because there are three or more consecutively coupled constraints

GPU-aware MPI was not detected, will not use direct GPU communication. Check the GROMACS install guide for recommendations for GPU-aware support. If you are certain about GPU-aware support in your MPI library, you can force its use by setting the GMX_FORCE_GPU_AWARE_MPI environment variable.

On host nid006956 1 GPU selected for this run.
Mapping of GPU IDs to the 4 GPU tasks in the 4 ranks on this node:
  PP:0,PP:0,PP:0,PME:0
PP tasks will do (non-perturbed) short-ranged and most bonded interactions on the GPU
PP task will update and constrain coordinates on the CPU
PME tasks will do all aspects on the GPU
Using 4 MPI processes
Using 64 OpenMP threads per MPI process


Overriding thread affinity set outside gmx mdrun
starting mdrun 'Protein'
300 steps,      0.8 ps.
step 0
step 100
step 200
vol 1.00  imb F  4% pme/F 0.68 step 300, remaining wall clock time:     0 s          


Dynamic load balancing report:
 DLB was permanently on during the run per user request.
 Average load imbalance: 3.8%.
 The balanceable part of the MD step is 42%, load imbalance is computed from this.
 Part of the total run time spent waiting due to load imbalance: 1.6%.
 Steps where the load balancing was limited by -rdd, -rcon and/or -dds: Z 0 %
 Average PME mesh/force load: 0.679
 Part of the total run time spent waiting due to PP/PME imbalance: 2.0 %


               Core t (s)   Wall t (s)        (%)
       Time:      111.689        0.442    25274.5
                 (ns/day)    (hour/ns)
Performance:      147.126        0.163

GROMACS reminds you: "These are Ideas, They are Not Lies" (Magnapop)

[WARNING] yaksa: 1 leaked handle pool objects
[WARNING] yaksa: 1 leaked handle pool objects
[WARNING] yaksa: 1 leaked handle pool objects
[WARNING] yaksa: 1 leaked handle pool objects
