An instance of this daemon is already running
An instance of this daemon is already running
An instance of this daemon is already running
                      :-) GROMACS - gmx mdrun, 2024.4 (-:

Executable:   /users/ealnuaim/spack/opt/spack/linux-neoverse_v2/gromacs-2024.4-hukb6h4khv76tinit4hyacke62g6lj5r/bin/gmx_mpi
Data prefix:  /users/ealnuaim/spack/opt/spack/linux-neoverse_v2/gromacs-2024.4-hukb6h4khv76tinit4hyacke62g6lj5r
Working dir:  /users/ealnuaim/gromacs_benchmarks/TestcaseA_benchmarks/gromacs_testcase4_testcaseA/run_39
Command line:
  gmx_mpi mdrun -s /users/ealnuaim/GROMACS_TestCaseA/ion_channel.tpr -ntomp 64 -bonded gpu -nb gpu -pme gpu -pin on -v -noconfout -dlb yes -nstlist 300 -gpu_id 0 -npme 1 -nsteps 300


Back Off! I just backed up md.log to ./#md.log.1#
Reading file /users/ealnuaim/GROMACS_TestCaseA/ion_channel.tpr, VERSION 2020.3 (single precision)
Note: file tpx version 119, software tpx version 134
Overriding nsteps with value passed on the command line: 300 steps, 0.75 ps
Changing nstlist from 10 to 300, rlist from 1 to 1.81


Update groups can not be used for this system because there are three or more consecutively coupled constraints

On host nid006887 1 GPU selected for this run.
Mapping of GPU IDs to the 4 GPU tasks in the 4 ranks on this node:
  PP:0,PP:0,PP:0,PME:0
PP tasks will do (non-perturbed) short-ranged and most bonded interactions on the GPU
PP task will update and constrain coordinates on the CPU
PME tasks will do all aspects on the GPU
Using 4 MPI processes
Using 64 OpenMP threads per MPI process


Overriding thread affinity set outside gmx mdrun

Back Off! I just backed up ener.edr to ./#ener.edr.1#
starting mdrun 'Protein'
300 steps,      0.8 ps.
step 0
step 100
step 200
vol 1.00  imb F  2% pme/F 0.27 step 300, remaining wall clock time:     0 s          


Dynamic load balancing report:
 DLB was permanently on during the run per user request.
 Average load imbalance: 1.8%.
 The balanceable part of the MD step is 46%, load imbalance is computed from this.
 Part of the total run time spent waiting due to load imbalance: 0.8%.
 Steps where the load balancing was limited by -rdd, -rcon and/or -dds: Z 0 %
 Average PME mesh/force load: 0.274
 Part of the total run time spent waiting due to PP/PME imbalance: 4.1 %


               Core t (s)   Wall t (s)        (%)
       Time:      201.447        0.796    25313.8
                 (ns/day)    (hour/ns)
Performance:       81.699        0.294

GROMACS reminds you: "Give a Man a Fish" (Arrested Development)

Cannot send command to MPS control daemon process
Cannot send command to MPS control daemon process
Cannot send command to MPS control daemon process
