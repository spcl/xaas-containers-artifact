                      :-) GROMACS - gmx mdrun, 2024.5 (-:

Executable:   /soft/applications/Gromacs/gromacs-2024.5/build/bin/gmx_mpi
Data prefix:  /soft/applications/Gromacs/gromacs-2024.5/build
Working dir:  /home/alokvk2
Command line:
  gmx_mpi mdrun -s /home/alokvk2/xaas-containers/data/GROMACS_TestCaseB/lignocellulose.tpr -ntomp 104 -nsteps 100

Reading file /home/alokvk2/xaas-containers/data/GROMACS_TestCaseB/lignocellulose.tpr, VERSION 2020.1 (single precision)
Note: file tpx version 119, software tpx version 134
GPU-aware MPI detected, but by default GROMACS will not make use the direct GPU communication capabilities of MPI. For improved performance try enabling the feature by setting the GMX_ENABLE_DIRECT_GPU_COMM environment variable.

Overriding nsteps with value passed on the command line: 100 steps, 0.2 ps
Changing nstlist from 10 to 25, rlist from 1.262 to 1.407


Update groups can not be used for this system because there are three or more consecutively coupled constraints

1 GPU selected for this run.
Mapping of GPU IDs to the 1 GPU task in the 1 rank on this node:
  PP:0
PP tasks will do (non-perturbed) short-ranged interactions on the GPU
PP task will update and constrain coordinates on the GPU
Using 1 MPI process

Non-default thread affinity set, disabling internal thread affinity

Using 104 OpenMP threads 

starting mdrun ''
100 steps,      0.2 ps.

Writing final coordinates.

               Core t (s)   Wall t (s)        (%)
       Time:     1431.542       13.767    10398.2
                 (ns/day)    (hour/ns)
Performance:        1.268       18.932

GROMACS reminds you: "I had trouble with physics in college. When I signed up I thought it said psychics." (Greg Tamblyn)

